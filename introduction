
# ðŸ§  **Pramitâ€™s AI & Deep Learning Master Notes (2025 Revision)**

---

## **1ï¸âƒ£ Machine Learning vs Deep Learning**

| Concept             | Machine Learning                                                               | Deep Learning                                                  |
| ------------------- | ------------------------------------------------------------------------------ | -------------------------------------------------------------- |
| Core Idea           | Learns from data using algorithms like linear regression, decision trees, etc. | Uses **multi-layer neural networks** to learn complex patterns |
| Feature Engineering | Manual                                                                         | Automatic (learned by the model)                               |
| Data Needs          | Moderate                                                                       | Huge datasets                                                  |
| Example             | Random Forest, SVM                                                             | CNN, RNN, Transformer                                          |

---

## **2ï¸âƒ£ Neural Networks (NNs)**

* Composed of **neurons (nodes)** arranged in **layers**:

  * Input â†’ Hidden Layers â†’ Output
* Each neuron performs:
  [
  y = f(Wx + b)
  ]
  where `f` is a **non-linear activation function**.
* **Activation Functions:**

  * **ReLU** â†’ used in deep nets to avoid vanishing gradients
  * **Sigmoid / Tanh** â†’ used in binary outputs or older RNNs

---

## **3ï¸âƒ£ Why Non-Linearity?**

* Linear models can only learn straight-line relationships.
* Real-world data is complex â†’ need **non-linear transformations** (ReLU, Sigmoid).
* Enables deep networks to learn *complex patterns* like images, language, and sound.

---

## **4ï¸âƒ£ Deep Learning Training Process**

* **Forward Propagation:** Input flows through layers â†’ output predicted.
* **Backward Propagation:** Error (loss) is sent backward â†’ weights updated.
* Uses **Gradient Descent** to minimize loss.

---

## **5ï¸âƒ£ Vanishing Gradient Problem**

* In deep networks or RNNs, gradients become very small as they flow backward.
* Early layers stop learning â†’ model stagnates.
* Solved using ReLU, Batch Normalization, and architectures like LSTM or ResNet.

---

## **6ï¸âƒ£ CNN (Convolutional Neural Networks)**

* Specialized for **images**.
* Uses **convolutional filters** to detect patterns (edges, textures, shapes).
* Key layers: Convolution â†’ Pooling â†’ Fully Connected â†’ Output.
* Example: Image classification, object detection.

---

## **7ï¸âƒ£ RNN (Recurrent Neural Networks)**

* Designed for **sequential data** (text, speech, time series).
* Maintains â€œmemoryâ€ of previous inputs using a **hidden state**.
* Formula:
  [
  h_t = f(Wx_t + Uh_{t-1} + b)
  ]
* Limitations:

  * Canâ€™t remember long-term dependencies (due to vanishing gradients).

---

## **8ï¸âƒ£ LSTM (Long Short-Term Memory)**

* Improved version of RNN.
* Introduces **gates** to control information flow:

  * **Forget Gate:** decides what to forget.
  * **Input Gate:** decides what new info to add.
  * **Output Gate:** decides what to output.
* Helps retain long-term memory (used in text, speech tasks before Transformers).

---

## **9ï¸âƒ£ Sequence Modeling**

* Goal: predict or generate data sequences (e.g., next word, stock price, sound wave).
* RNNs/LSTMs handle this but canâ€™t parallelize easily.
* Led to the invention of **Transformers**, which model sequences without recurrence.

---

## **ðŸ”Ÿ Attention Mechanism**

> â€œDonâ€™t look at everything, focus on what matters.â€

* Instead of processing one step at a time (like RNN), Attention lets the model look at **all words at once** and weigh their importance.
* Example:

  * In â€œThe cat sat on the mat,â€ the word â€œcatâ€ relates more to â€œsatâ€ than â€œmat.â€
* Improves long-term context understanding.

---

## **1ï¸âƒ£1ï¸âƒ£ Transformers (2017 â€“ Present)**

* Introduced in paper *â€œAttention is All You Needâ€*.
* Replaced RNN/LSTM with **self-attention** mechanism.
* Fully parallelizable â†’ much faster training.
* Core parts:

  1. **Embedding Layer:** Converts words â†’ numerical vectors.
  2. **Positional Encoding:** Adds position info since Transformer doesnâ€™t process sequentially.
  3. **Self-Attention:** Computes relationships between all words in a sentence.
  4. **Feed-Forward Layers:** Apply nonlinear transformations.
  5. **Residual Connections + Layer Norm:** Helps stable training.

---

## **1ï¸âƒ£2ï¸âƒ£ Embeddings**

* Words or tokens are represented as **dense vectors** (e.g., 300 dimensions).
* Words with similar meanings have **similar embeddings**.
* Example: vector(â€œkingâ€) - vector(â€œmanâ€) + vector(â€œwomanâ€) â‰ˆ vector(â€œqueenâ€)
* Created using models like Word2Vec, GloVe, or learned directly in Transformers.

---

## **1ï¸âƒ£3ï¸âƒ£ Generative AI (GenAI)**

Two major categories:

| Type                           | Example Models          | Generates            |
| ------------------------------ | ----------------------- | -------------------- |
| **Generative Language Models** | GPT, Claude, Gemini     | Text, code, dialogue |
| **Generative Image Models**    | GANs, Diffusion, DALLÂ·E | Images, art, design  |

---

## **1ï¸âƒ£4ï¸âƒ£ GAN (Generative Adversarial Network)**

* Has **Generator** (creates fake images) and **Discriminator** (detects fakes).
* Trained in a *competition* until generator produces realistic outputs.
* Used for:

  * Face generation (ThisPersonDoesNotExist)
  * Image synthesis & editing
  * Data augmentation

---

## **1ï¸âƒ£5ï¸âƒ£ Diffusion Models**

* Modern image generators (e.g., **Stable Diffusion**, **DALLÂ·E 3**).
* Work by:

  1. Adding random noise to an image.
  2. Learning how to remove noise step by step.
* No separate generator/discriminator â€” itâ€™s a *probabilistic denoising process*.

---

## **1ï¸âƒ£6ï¸âƒ£ Large Language Models (LLMs)**

* Advanced **Transformer-based** deep learning models.
* Trained on massive text datasets to understand and generate language.

**Training Stages:**

1. **Pretraining (Self-Supervised):** Predict next word â€” learns language patterns.
2. **Supervised Fine-Tuning:** Learns to follow human instructions.
3. **RLHF (Reinforcement Learning from Human Feedback):** Optimizes behavior based on human preferences.

**Capabilities:**

* Text generation, summarization, translation, code writing, reasoning.

---

## **1ï¸âƒ£7ï¸âƒ£ Modern AI Evolution Timeline**

| Year    | Model Type      | Example                            | Focus                      |
| ------- | --------------- | ---------------------------------- | -------------------------- |
| 2014    | GAN             | StyleGAN                           | Image generation           |
| 2015â€“17 | RNN/LSTM        | seq2seq                            | Sequence modeling          |
| 2017    | Transformer     | â€œAttention is All You Needâ€        | Parallel sequence modeling |
| 2019    | GPT-2           | Text generation                    |                            |
| 2020+   | GPT-3, BERT     | Massive pretrained language models |                            |
| 2022+   | Diffusion       | Stable Diffusion, Midjourney       |                            |
| 2023+   | Multimodal LLMs | GPT-4o, Gemini                     | Text + image + audio       |

---

## **1ï¸âƒ£8ï¸âƒ£ Key Concepts to Revise**

âœ… Forward & Backward Propagation
âœ… Gradient Descent & Optimizers (Adam, SGD)
âœ… Overfitting vs Underfitting
âœ… Dropout & Regularization
âœ… Transfer Learning
âœ… Tokenization (for text input)
âœ… Beam Search / Sampling (in text generation)
âœ… Evaluation Metrics (Accuracy, Precision, Recall, BLEU, Perplexity)

---

## **1ï¸âƒ£9ï¸âƒ£ GPU Importance**

* GPUs accelerate matrix operations used in neural networks.
* Your **GTX 1650** is great for:

  * CNNs, RNNs, small transformer training, inference.
* For large-scale LLMs â†’ use **cloud GPUs** (A100, H100, TPU).

---

### ðŸ§© **In one line summary:**

> Deep Learning â†’ Neural Networks â†’ CNNs/RNNs/LSTMs â†’ Attention â†’ Transformers â†’ LLMs â†’ GenAI.

---

Would you like me to convert this into a **nicely formatted PDF (with section headings & clean design)**
so you can use it as your **personal AI Revision Notes (2025 Edition)**?
